{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes by - Kiran A Bendigeri\n",
    "Please Read 'Read me' file.\n",
    "\n",
    "1 OpenCV with Python Intro and loading Images tutorial\n",
    "OpenCV is used for all sorts of image and video analysis, like facial recognition and detection, license plate reading, photo editing, advanced robotic vision, optical character recognition, and a whole lot more.\n",
    "You will need two main libraries, with an optional third: python-OpenCV, Numpy, and Matplotlib.\n",
    "\n",
    "First, we should understand a few basic assumptions and paradigms when it comes to image and video analysis. With the way just about every video camera records today, recordings are actually frames, displayed one after another, 30-60+ times a second. At the core, however, they are static frames, just like images. Thus, image recognition and video analysis use identical methods for the most part. Some things, like directional tracking, is going to require a succession of images (frames), but something like facial detection, or object recognition can be done with almost the exact same code on images and video.\n",
    "\n",
    "Next, a lot of image and video analysis boils down to simplifying the source as much as possible. This almost always begins with a conversion to grayscale, but it can also be a color filter, gradient, or a combination of these. From here, we can do all sorts of analysis and transformations to the source. Generally, what winds up happening is there is a transformation done, then analysis, then any overlays that we wish to apply are applied back to the original source, which is why you can often see the \"finished product\" of maybe object or facial recognition being shown on a full-color image or video. Rarely is the data actually processed in raw form like this, however. Some examples of what we can do at a basic level. All of these are done with a basic web cam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " we define img to be cv2.read(image file, parms). The default is going to be IMREAD_COLOR, which is color without any alpha channel. If you're not familiar, alpha is the degree of opaqueness (the opposite of transparency). If you need to retain the alpha channel, you can also use IMREAD_UNCHANGED. Many times, you will be reading in the color version, and later converting it to gray. If you do not have a webcam, this will be the main method you will use throughout this tutorial, loading an image.\n",
    "\n",
    "Rather than using IMREAD_COLOR...etc, you can also use simple numbers. You should be familiar with both options, so you understand what the person is doing. For the second parameter, you can use -1, 0, or 1. Color is 1, grayscale is 0, and the unchanged is -1. Thus, for grayscale, one could do img = cv2.imread('watch.jpg', 0)\n",
    "\n",
    "Once loaded, we use cv2.imshow(title,image) to show the image. From here, we use the cv2.waitKey(0) to wait until any key is pressed. Once that's done, we use cv2.destroyAllWindows() to close everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# you can save\n",
    "cv2.imwrite('watchgray.png',img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also display images with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "plt.imshow(img, cmap = 'gray', interpolation = 'bicubic')\n",
    "plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "plt.plot([200,300,400],[100,200,300],'c', linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Loading Video Source OpenCV Python Tutorial\n",
    "Handling frames from a video is identical to handling for images. \n",
    "cap = cv2.VideoCapture(0). This will return video from the first webcam on your computer. \n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "This code initiates an infinite loop (to be broken later by a break statement), where we have ret and frame being defined as the cap.read(). Basically, ret is a boolean regarding whether or not there was a return at all, at the frame is each frame that is returned. If there is no frame, you wont get an error, you will get None.\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "Here, we define a new variable, gray, as the frame, converted to gray. Notice this says BGR2GRAY. It is important to note that OpenCV reads colors as BGR (Blue Green Red), where most computer applications read as RGB (Red Green Blue). Remember this.\n",
    "\n",
    "    cv2.imshow('frame',gray)\n",
    "Notice that, despite being a video stream, we still use imshow. Here, we're showing the converted-to-gray feed. If you wish to show both at the same time, you can do imshow for the original frame, and imshow for the gray and two windows will appear.\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "This statement just runs once per frame. Basically, if we get a key, and that key is a q, we will exit the while loop with a break, which then runs:\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "This releases the webcam, then closes all of the imshow() windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "while(True):\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    out.write(frame)\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Drawing and Writing on Image OpenCV Python Tutorial\n",
    "The cv2.line() takes the following parameters: where, start coordinates, end coordinates, color (bgr), line thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "img = cv2.imread('watch.jpg',cv2.IMREAD_COLOR)\n",
    "cv2.line(img,(0,0),(200,300),(255,255,255),50)\n",
    "cv2.rectangle(img,(500,250),(1000,500),(0,0,255),15)\n",
    "cv2.circle(img,(447,63), 63, (0,255,0), -1)\n",
    "pts = np.array([[100,50],[200,300],[700,200],[500,100]], np.int32)\n",
    "pts = pts.reshape((-1,1,2))\n",
    "cv2.polylines(img, [pts], True, (0,255,255), 3)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(img,'OpenCV Tuts!',(10,500), font, 6, (200,255,155), 13, cv2.LINE_AA)\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Image Operations OpenCV Python Tutorial\n",
    "Every video breaks down into frames. Each frame, like an image, then breaks down into pixels stored in rows and columns within the frame/picture. Each pixel has a coordinate location, and each pixel is comprised of color values. \n",
    "Now, we can reference specific pixels, like so:\n",
    "\n",
    "px = img[55,55]\n",
    "Next, we could actually change a pixel:\n",
    "\n",
    "img[55,55] = [255,255,255]\n",
    "Then re-reference:\n",
    "\n",
    "px = img[55,55]\n",
    "print(px)\n",
    "It should be different now. Next, we can reference an ROI, or Region of Image, like so:\n",
    "\n",
    "px = img[100:150,100:150]\n",
    "print(px)\n",
    "We can also modify the ROI, like this:\n",
    "\n",
    "img[100:150,100:150] = [255,255,255]\n",
    "We can reference certain characteristics of our image:\n",
    "\n",
    "print(img.shape)\n",
    "print(img.size)\n",
    "print(img.dtype)\n",
    "And we can perform operations, like:\n",
    "\n",
    "watch_face = img[37:111,107:194]\n",
    "img[0:74,0:87] = watch_face\n",
    "\n",
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 Image arithmetics and Logic OpenCV Python Tutorial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 500 x 250\n",
    "img1 = cv2.imread('3D-Matplotlib.png')\n",
    "img2 = cv2.imread('mainsvmimage.png')\n",
    "\n",
    "add = img1+img2\n",
    "\n",
    "cv2.imshow('add',add)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = cv2.addWeighted(img1, 0.6, img2, 0.4, 0)\n",
    "cv2.imshow('weighted',weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load two images\n",
    "img1 = cv2.imread('3D-Matplotlib.png')\n",
    "img2 = cv2.imread('mainlogo.png')\n",
    "\n",
    "# I want to put logo on top-left corner, So I create a ROI\n",
    "rows,cols,channels = img2.shape\n",
    "roi = img1[0:rows, 0:cols ]\n",
    "\n",
    "# Now create a mask of logo and create its inverse mask\n",
    "img2gray = cv2.cvtColor(img2,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# add a threshold\n",
    "ret, mask = cv2.threshold(img2gray, 220, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "# Now black-out the area of logo in ROI\n",
    "img1_bg = cv2.bitwise_and(roi,roi,mask = mask_inv)\n",
    "\n",
    "# Take only region of logo from logo image.\n",
    "img2_fg = cv2.bitwise_and(img2,img2,mask = mask)\n",
    "\n",
    "dst = cv2.add(img1_bg,img2_fg)\n",
    "img1[0:rows, 0:cols ] = dst\n",
    "\n",
    "cv2.imshow('res',img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 Thresholding OpenCV Python Tutorial\n",
    "The idea of thresholding is to further-simplify visual data for analysis. First, you may convert to gray-scale, but then you have to consider that grayscale still has at least 255 values. What thresholding can do, at the most basic level, is convert everything to white or black, based on a threshold value. Let's say we want the threshold to be 125 (out of 255), then everything that was 125 and under would be converted to 0, or black, and everything above 125 would be converted to 255, or white. If you convert to grayscale as you normally will, you will get white and black. If you do not convert to grayscale, you will get thresholded pictures, but there will be color.\n",
    "A binary threshold is a simple \"either or\" threshold, where the pixels are either 255 or 0. In many cases, this would be white or black, but we have left our image colored for now, so it may be colored still. The first parameter here is the image. The next parameter is the threshold, we are choosing 10. The next is the maximum value, which we're choosing as 255. Next and finally we have the type of threshold, which we've chosen as THRESH_BINARY. Normally, a threshold of 10 would be somewhat poor of a choice. We are choosing 10, because this is a low-light picture, so we choose a low number. Normally something about 125-150 would probably work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('bookpage.jpg')\n",
    "retval, threshold = cv2.threshold(img, 12, 255, cv2.THRESH_BINARY)\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('threshold',threshold)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "th = cv2.adaptiveThreshold(grayscaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('Adaptive threshold',th)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 Color Filtering OpenCV Python Tutorial\n",
    " you are probably going to convert your colors to HSV, which is \"Hue Saturation Value.\" This can help you actually pinpoint a more specific color, based on hue and saturation ranges, with a variance of value, for example. If you wanted, you could actually produce filters based on BGR values, but this would be a bit more difficult. If you're having a hard time visualizing HSV, don't feel silly, check out the Wikipedia page on HSV, there is a very useful graphic there for you to visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    cv2.imshow('frame',frame)\n",
    "    cv2.imshow('mask',mask)\n",
    "    cv2.imshow('res',res)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 Blurring and Smoothing OpenCV Python Tutorial\n",
    "we're going to be covering how to try to eliminate noise from our filters, like simple thresholds or even a specific color filter like we had before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "    \n",
    "    kernel = np.ones((15,15),np.float32)/225\n",
    "    smoothed = cv2.filter2D(res,-1,kernel)\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Averaging',smoothed)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv2.GaussianBlur(res,(15,15),0)\n",
    "cv2.imshow('Gaussian Blurring',blur)\n",
    "    \n",
    "median = cv2.medianBlur(res,15)\n",
    "cv2.imshow('Median Blur',median)  \n",
    "\n",
    "bilateral = cv2.bilateralFilter(res,15,75,75)\n",
    "cv2.imshow('bilateral Blur',bilateral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 Morphological Transformations OpenCV Python Tutorial\n",
    "These are some simple operations that we can perform based on the image's shape.\n",
    "\n",
    "These tend to come in pairs. The first pair we're going to talk about is Erosion and Dilation. Erosion is where we will \"erode\" the edges. The way these work is we work with a slider (kernel). We give the slider a size, let's say 5 x 5 pixels. What happens is we slide this slider around, and if all of the pixels are white, then we get white, otherwise black. This may help eliminate some white noise. The other version of this is Dilation, which basically does the opposite: Slides around, if the entire area isn't black, then it is converted to white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    kernel = np.ones((5,5),np.uint8)\n",
    "    \n",
    "    opening = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "    closing = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    cv2.imshow('Mask',mask)\n",
    "    cv2.imshow('Opening',opening)\n",
    "    cv2.imshow('Closing',closing)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two other options that aren't really useful for our case here are \"tophat\" and \"blackhat:\"\n",
    "\n",
    "    # It is the difference between input image and Opening of the image\n",
    "    cv2.imshow('Tophat',tophat)\n",
    "\n",
    "    # It is the difference between the closing of the input image and input image.\n",
    "    cv2.imshow('Blackhat',blackhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 Canny Edge Detection and Gradients OpenCV Python Tutorial\n",
    "Image gradients can be used to measure directional intensity, and edge detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(1):\n",
    "\n",
    "    _, frame = cap.read()\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    lower_red = np.array([30,150,50])\n",
    "    upper_red = np.array([255,255,180])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    res = cv2.bitwise_and(frame,frame, mask= mask)\n",
    "\n",
    "    cv2.imshow('Original',frame)\n",
    "    edges = cv2.Canny(frame,100,200)\n",
    "    cv2.imshow('Edges',edges)\n",
    "\n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11 Haar Cascade Object Detection Face & Eye OpenCV Python Tutorial\n",
    "In this OpenCV with Python tutorial, we're going to discuss object detection with Haar Cascades. We'll do face and eye detection to start. In order to do object recognition/detection with cascade files, you first need cascade files. For the extremely popular tasks, these already exist. Detecting things like faces, cars, smiles, eyes, and license plates for example are all pretty prevalent.\n",
    "\n",
    "First, I will show you how to use these cascade files, then I will show you how to embark on creating your very own cascades, so that you can detect any object you want, which is pretty darn cool!\n",
    "\n",
    "You can use Google to find various Haar Cascades of things you may want to detect. You shouldn't have too much trouble finding the aforementioned types. We will use a Face cascade and Eye cascade. You can find a few more at the root directory of Haar cascades. Note the license for using/distributing these Haar Cascades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# multiple cascades: https://github.com/Itseez/opencv/tree/master/data/haarcascades\n",
    "\n",
    "#https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "#https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_eye.xml\n",
    "eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while 1:\n",
    "    ret, img = cap.read()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "        \n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "\n",
    "    cv2.imshow('img',img)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
